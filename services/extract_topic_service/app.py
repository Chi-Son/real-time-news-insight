import json
import logging
import os
import re
import torch
import psycopg2
from psycopg2.extras import DictCursor
from flashtext import KeywordProcessor
from sentence_transformers import SentenceTransformer, util
from shared.kafka_config import get_kafka_consumer,get_kafka_producer
from shared.postgresql_config import DB_CONFIG
import ast
import numpy as np
import time
import copy


# =========================
# LOGGING
# =========================
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("topic_ranking")

# =========================
# KAFKA CONFIG
# =========================
INPUT_TOPIC = "extractor_news"
GROUP_ID = "topic_ranking_group"
OUTPUT_TOPIC = "extractor_topic"
producer = get_kafka_producer()
consumer = get_kafka_consumer(topic=INPUT_TOPIC, group_id=GROUP_ID)
BATCH_SIZE = 10
BATCH_TIMEOUT = 20 
batch_messages = []
last_flush_time = time.time()

# =========================
# COUNTRY STOPWORDS
# =========================
COUNTRY_STOPWORDS = [
    "phÃ¡p lÃ½", "tÆ° phÃ¡p", "luáº­t phÃ¡p","phÃ¡p luáº­t",
    "Ä‘á»©c tÃ­nh", "Ä‘á»©c háº¡nh",
    "nga ngá»‘", "nga ngÃ¡o",
    "má»¹ pháº©m","má»¹ thuáº­t","má»¹ Ä‘Ã¬nh","má»¹ thuáº­t", "má»¹ nhÃ¢n", "má»¹ miá»u", "mÄ© mÃ£n", "mÄ© vá»‹", "mÄ© thuáº­t", "má»¹ viá»‡n", "má»¹ mÃ£n"
]
def is_valid_proper_noun(entity_text: str, full_text: str) -> bool:
    """
    Chá»‰ cháº¥p nháº­n entity náº¿u:
    - Token Ä‘áº§u viáº¿t hoa
    - KhÃ´ng pháº£i danh tá»« chung viáº¿t thÆ°á»ng
    - Náº¿u nhiá»u tá»« â†’ má»—i tá»« pháº£i viáº¿t hoa chá»¯ cÃ¡i Ä‘áº§u
    """

    words = entity_text.split()

    # 1. Reject náº¿u toÃ n bá»™ lÃ  lowercase
    if entity_text.islower():
        return False

    # 2. Vá»›i multi-word: má»—i tá»« pháº£i Capitalized
    for w in words:
        if not re.match(r"^[A-ZÄ][a-zA-ZÃ€-á»´Ã -á»µ]+$", w):
            return False

    # 3. KhÃ´ng cho Ä‘á»©ng sau cÃ¡c tá»« chá»‰ loáº¡i chung
    COMMON_PREFIX = [
        "ngÃ nh", "sáº£n pháº©m", "lÄ©nh vá»±c", "máº£ng", "loáº¡i",
        "cÃ¡c", "nhá»¯ng"
    ]

    text_lower = full_text.lower()
    for p in COMMON_PREFIX:
        if f"{p} {entity_text.lower()}" in text_lower:
            return False

    return True

# =========================
# PERSON NAME CHECK
# =========================
def is_person_name_around(entity_text, full_text):
    name = entity_text
    pattern_mid = rf"\b[A-ZÄ][a-zA-ZÃ€-á»´Ã -á»µ]+ {name} [A-ZÄ][a-zA-ZÃ€-á»´Ã -á»µ]+\b"
    pattern_after = rf"\b[A-ZÄ][a-zA-ZÃ€-á»´Ã -á»µ]+ {name}\b"
    pattern_before = rf"\b{name} [A-ZÄ][a-zA-ZÃ€-á»´Ã -á»µ]+\b"

    return (
        re.search(pattern_mid, full_text)
        or re.search(pattern_after, full_text)
        or re.search(pattern_before, full_text)
    )

# =========================
# FILTER COUNTRY FALSE MATCH
# =========================
def filter_country_false_matches(entities, text):
    filtered = []
    text_lower = text.lower()

    for ent in entities:
        # Giá»¯ nguyÃªn object 'ent' Ä‘á»ƒ khÃ´ng máº¥t entity_id
        label = ent["label"]
        ent_text = ent["text"]

        if label in ("COUNTRY", "ORG", "PERSON"):
            if not is_valid_proper_noun(ent_text, text):
                continue

        if label == "COUNTRY":
            if any(sw in text_lower for sw in COUNTRY_STOPWORDS):
                continue
            if is_person_name_around(ent_text, text):
                continue

        # Append nguyÃªn object ban Ä‘áº§u vÃ o
        filtered.append(ent) 

    return filtered


# =========================
# KEYWORD PROCESSOR
# =========================
keyword_processor = KeywordProcessor(case_sensitive=True)
viet_chars = set(
    "Ã Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…"
    "Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡"
    "Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘"
    "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"
)
keyword_processor.non_word_boundaries = viet_chars


# =========================
# LOAD NER DATA + ENTITY ID
# =========================
ENTITY_ID_MAP = {}  # (text, type) -> entity_id

def load_ner_data_from_db():
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor(cursor_factory=DictCursor)

    cur.execute("SELECT entity_id, type, text, aliases FROM entity;")
    count = 0

    for row in cur.fetchall():
        key = (row["text"], row["type"])
        ENTITY_ID_MAP[key] = row["entity_id"]

        keyword_processor.add_keyword(
            row["text"],
            {
                "label": row["type"],
                "text": row["text"],
                "entity_id": row["entity_id"]
            }
        )
        count += 1

        aliases = row["aliases"] or []
        if not isinstance(aliases, list):
            aliases = json.loads(aliases)

        for a in aliases:
            keyword_processor.add_keyword(
                a,
                {
                    "label": row["type"],
                    "text": row["text"],   # canonical text
                    "entity_id": row["entity_id"]
                }
            )
            count += 1

    conn.close()
    logger.info(f"Loaded {count} NER keywords with entity_id")


load_ner_data_from_db()

# =========================
# ENTITY + CLEAN TEXT
# =========================
def extract_entities_and_clean(text: str):
    """
    - Clone entity dict ngay khi extract (trÃ¡nh FlashText reuse object)
    - Ã‰p kiá»ƒu entity_id
    - Deduplicate theo (text, label)
    """

    matches = keyword_processor.extract_keywords(text, span_info=True)

    unique = {}

    for m in matches:
        raw_ent = m[0]  # dict gá»‘c tá»« FlashText (KHÃ”NG Ä‘Æ°á»£c dÃ¹ng trá»±c tiáº¿p)

        # --- CLONE Sáº CH ---
        ent = {
            "label": raw_ent.get("label"),
            "text": raw_ent.get("text"),
            "entity_id": (
                int(raw_ent["entity_id"])
                if raw_ent.get("entity_id") is not None
                else None
            )
        }

        key = (ent["text"], ent["label"])
        if key not in unique:
            unique[key] = ent

    entities = filter_country_false_matches(
        list(unique.values()),
        text
    )

    clean_text = " ".join(text.split())
    return entities, clean_text




# =========================
# LOAD TOPIC EMBEDDINGS
# =========================
def load_topic_embeddings():
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor(cursor_factory=DictCursor)

    cur.execute("""
        SELECT t.topic_id, t.name, e.embedding
        FROM topic_embedding e
        JOIN topic t ON t.topic_id = e.topic_id
        ORDER BY t.topic_id
    """)

    topic_ids = []
    topic_names = []
    vectors = []

    for row in cur.fetchall():
        topic_ids.append(row["topic_id"])
        topic_names.append(row["name"])

        emb = row["embedding"]

        # pgvector â†’ Python
        if not isinstance(emb, (list, tuple)):
            emb = ast.literal_eval(emb)

        vectors.append(emb)

    conn.close()

    embeddings = torch.from_numpy(
        np.asarray(vectors, dtype="float32")
    )
    embeddings = torch.nn.functional.normalize(embeddings, dim=1)

    logger.info(f"Loaded {len(topic_ids)} topic embeddings")
    return topic_ids, topic_names, embeddings

topic_ids, topic_names, topic_embeddings = load_topic_embeddings()

# =========================
# LOAD ARTICLE MODEL
# =========================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_PATH = os.path.join(BASE_DIR, "models", "ModelPhoberSim")

model = SentenceTransformer(MODEL_PATH)
logger.info("SentenceTransformer loaded")

# =========================
# ARTICLE TEXT FOR RANKING
# =========================
def get_text_for_ranking(title, content):
    sentences = re.split(r"(?<=[.!?])\s+", content)
    return f"{title}. {' '.join(sentences[:3])}"

logger.info("ðŸš€ Topic Ranking Service started")

# =========================
# MAIN LOOP
# =========================

while True:
    msg = consumer.poll(1.0)
    current_time = time.time()

    # Kiá»ƒm tra timeout batch
    if batch_messages and (len(batch_messages) >= BATCH_SIZE or current_time - last_flush_time >= BATCH_TIMEOUT):
        for m in batch_messages:
            producer.produce(
                OUTPUT_TOPIC,
                key=str(m["id"]),
                value=json.dumps(m).encode("utf-8")
            )
        producer.flush()
        batch_messages.clear()
        last_flush_time = current_time

    if msg is None:
        continue
    if msg.error():
        logger.error(msg.error())
        continue

    try:
        data = json.loads(msg.value().decode("utf-8"))
        news_id = data.get("id")
        title = data.get("title")
        content = data.get("content")

        if not title or not content:
            continue

        text = get_text_for_ranking(title, content)
        entities, clean_text = extract_entities_and_clean(text)

        article_embedding = model.encode(
            [clean_text],
            convert_to_tensor=True,
            normalize_embeddings=True,
            show_progress_bar=False
        )

        scores = util.cos_sim(article_embedding, topic_embeddings)
        top_k = min(3, len(topic_ids))
        top_scores, top_indices = torch.topk(scores[0], k=top_k)
        matched_topic_ids = [
            topic_ids[top_indices[i].item()] 
            for i in range(top_k) 
            if top_scores[i].item() > 0.65
        ]
        if matched_topic_ids:
                    logger.info(f"[NEWS {news_id}] '{title}' â†’ Topics: {matched_topic_ids}")
                    logger.info(f"[NEWS {news_id}] ENTITIES RAW = {entities}")
                    msg_out = {
                        "id": news_id,
                        "title": title,
                        "content": content,
                        "published_at": data.get("published_at"),
                        "topic_ids": matched_topic_ids,  
                        "entities": entities 
                    }
                    batch_messages.append(msg_out)
                    for e in entities:
                        if e.get("entity_id") is None:
                            logger.error(
                                f"[NEWS {news_id}] ENTITY LOST ID BEFORE KAFKA: {e}"
                            )

    except Exception as e:
        logger.exception(e)