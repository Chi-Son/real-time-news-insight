import json
import logging
import math
import time
import requests
import os
from datetime import datetime, timezone

from shared.kafka_config import get_kafka_consumer
from shared.redis_connect import redis_connection
import requests

import psycopg2
from shared.postgresql_config import DB_CONFIG
# =========================
# LOGGING
# =========================
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("topic_scoring_24h")

WEBSOCKET_PUSH_URL = os.getenv(
    "WEBSOCKET_PUSH_URL",
    "http://websocket_service:8000/ws-push/topics"
)
# =========================
# KAFKA CONFIG
# =========================
INPUT_TOPIC = "extractor_sentiment"
GROUP_ID = "topic_scoring_24h_group"

consumer = get_kafka_consumer(
    topic=INPUT_TOPIC,
    group_id=GROUP_ID
)

# =========================
# REDIS
# =========================
redis = redis_connection()

# =========================
# CONFIG
# =========================
WINDOW_HOURS = 24
WINDOW_MINUTES = WINDOW_HOURS * 60        
REDIS_TTL_SECONDS = 48 * 3600               
DECAY_LAMBDA = 0.0048                       


def load_topic_map():
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()
    cur.execute("SELECT topic_id, name FROM topic")
    rows = cur.fetchall()
    cur.close()
    conn.close()

    return {str(topic_id): name for topic_id, name in rows}

TOPIC_NAME_MAP = load_topic_map()
# =========================
# UTILS
# =========================
def to_epoch(ts: str) -> int:
    """
    ISO8601 -> epoch seconds (UTC)
    """
    dt = datetime.fromisoformat(ts.replace("Z", "+00:00"))
    return int(dt.astimezone(timezone.utc).timestamp())


def decay_score(minutes_diff: float) -> float:
    """
    Exponential decay theo ph√∫t
    """
    return math.exp(-DECAY_LAMBDA * minutes_diff)

def push_topic_scores(redis):
    """
    L·∫•y to√†n b·ªô topic score, sort desc, push websocket
    """
    raw = redis.zrevrange("topic:score", 0, -1, withscores=True)

    topics = []
    for topic_id, score in raw:
        tid = topic_id.decode() if isinstance(topic_id, bytes) else topic_id
        topics.append({
            "topic_id": tid,
            "topic_name": TOPIC_NAME_MAP.get(tid, f"Topic {tid}"),
            "score": round(score, 6)
        })

    payload = {
        "type": "topic_score_update",
        "data": topics,
        "updated_at": int(time.time())
    }

    try:
        requests.post(WEBSOCKET_PUSH_URL, json=payload, timeout=0.5)
    except Exception as e:
        logger.warning(f"WebSocket push failed: {e}")

def cleanup_old_articles(redis, topic_id: str, now_epoch: int):
    """
    X√≥a c√°c b√†i vi·∫øt c≈© h∆°n 24h kh·ªèi Redis sorted set
    """
    topic_key = f"topic:window:{topic_id}"
    cutoff_epoch = now_epoch - (WINDOW_MINUTES * 60)
    
    # X√≥a t·∫•t c·∫£ b√†i vi·∫øt c√≥ timestamp < cutoff
    removed = redis.zremrangebyscore(topic_key, '-inf', cutoff_epoch)
    
    if removed > 0:
        logger.info(f"[CLEANUP] Removed {removed} old articles from topic {topic_id}")
    
    return removed

def cleanup_all_topics(redis):
    """
    D·ªçn d·∫πp t·∫•t c·∫£ topics, x√≥a b√†i c≈© v√† c·∫≠p nh·∫≠t ƒëi·ªÉm
    """
    now_epoch = int(time.time())
    cutoff_epoch = now_epoch - (WINDOW_MINUTES * 60)
    
    # L·∫•y t·∫•t c·∫£ topic keys
    topic_keys = redis.keys("topic:window:*")
    
    for topic_key in topic_keys:
        topic_key_str = topic_key.decode() if isinstance(topic_key, bytes) else topic_key
        topic_id = topic_key_str.replace("topic:window:", "")
        
        # X√≥a b√†i c≈©
        removed = redis.zremrangebyscore(topic_key_str, '-inf', cutoff_epoch)
        
        # L·∫•y c√°c b√†i c√≤n l·∫°i
        items = redis.zrange(topic_key_str, 0, -1, withscores=True)
        
        if not items:
            # Topic kh√¥ng c√≤n b√†i n√†o, x√≥a lu√¥n
            redis.delete(topic_key_str)
            redis.zrem("topic:score", str(topic_id))
            logger.info(f"[CLEANUP] Deleted empty topic {topic_id}")
            continue
        
        # T√≠nh l·∫°i ƒëi·ªÉm
        total_score = 0.0
        for _, ts in items:
            minutes_diff = (now_epoch - ts) / 60
            if minutes_diff <= WINDOW_MINUTES:
                total_score += decay_score(minutes_diff)
        
        total_score = round(total_score, 6)
        
        # C·∫≠p nh·∫≠t score
        redis.zadd("topic:score", {str(topic_id): total_score})
        
        if removed > 0:
            logger.info(
                f"[CLEANUP] Topic {topic_id}: removed {removed} old articles, "
                f"remaining {len(items)}, score={total_score}"
            )
    
    # Push c·∫≠p nh·∫≠t sau khi cleanup
    push_topic_scores(redis)

# =========================
# STARTUP CLEANUP
# =========================
def startup_cleanup(redis):
    """
    D·ªçn d·∫πp to√†n b·ªô d·ªØ li·ªáu c≈© khi service kh·ªüi ƒë·ªông
    Gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ: Redis b·ªã t·∫Øt ‚Üí TTL kh√¥ng ch·∫°y ‚Üí data c≈© t·ªìn ƒë·ªçng
    """
    logger.info("=" * 60)
    logger.info("üßπ STARTUP CLEANUP: Cleaning old data from Redis...")
    logger.info("=" * 60)
    
    now_epoch = int(time.time())
    cutoff_epoch = now_epoch - (WINDOW_MINUTES * 60)
    
    # L·∫•y t·∫•t c·∫£ topic keys
    topic_keys = redis.keys("topic:window:*")
    
    total_removed = 0
    total_topics = len(topic_keys)
    empty_topics = 0
    
    for topic_key in topic_keys:
        topic_key_str = topic_key.decode() if isinstance(topic_key, bytes) else topic_key
        topic_id = topic_key_str.replace("topic:window:", "")
        
        # X√≥a b√†i c≈© h∆°n 24h
        removed = redis.zremrangebyscore(topic_key_str, '-inf', cutoff_epoch)
        total_removed += removed
        
        # L·∫•y c√°c b√†i c√≤n l·∫°i
        items = redis.zrange(topic_key_str, 0, -1, withscores=True)
        
        if not items:
            # Topic kh√¥ng c√≤n b√†i n√†o, x√≥a lu√¥n
            redis.delete(topic_key_str)
            redis.zrem("topic:score", str(topic_id))
            empty_topics += 1
            logger.info(f"  ‚ùå Deleted empty topic {topic_id}")
            continue
        
        # T√≠nh l·∫°i ƒëi·ªÉm cho topic c√≤n data
        total_score = 0.0
        for _, ts in items:
            minutes_diff = (now_epoch - ts) / 60
            if minutes_diff <= WINDOW_MINUTES:
                total_score += decay_score(minutes_diff)
        
        total_score = round(total_score, 6)
        redis.zadd("topic:score", {str(topic_id): total_score})
        
        logger.info(
            f"  ‚úì Topic {topic_id}: removed {removed} old articles, "
            f"kept {len(items)}, score={total_score}"
        )
    
    logger.info("=" * 60)
    logger.info(f"‚úÖ STARTUP CLEANUP COMPLETE:")
    logger.info(f"   - Total topics processed: {total_topics}")
    logger.info(f"   - Empty topics deleted: {empty_topics}")
    logger.info(f"   - Old articles removed: {total_removed}")
    logger.info("=" * 60)
    
    # Push d·ªØ li·ªáu s·∫°ch l√™n WebSocket
    push_topic_scores(redis)

# =========================
# MAIN LOOP
# =========================
logger.info("üöÄ Topic Scoring 24h Service started")

# Ch·∫°y cleanup khi kh·ªüi ƒë·ªông
startup_cleanup(redis)

# Bi·∫øn ƒë·∫øm ƒë·ªÉ ch·∫°y cleanup ƒë·ªãnh k·ª≥
message_count = 0
CLEANUP_INTERVAL = 100  # Cleanup sau m·ªói 100 messages

while True:
    msg = consumer.poll(1.0)

    if msg is None:
        continue
    if msg.error():
        logger.error(msg.error())
        continue

    try:
        message_count += 1
        
        # Ch·∫°y cleanup ƒë·ªãnh k·ª≥
        if message_count % CLEANUP_INTERVAL == 0:
            logger.info(f"[PERIODIC CLEANUP] Running cleanup after {message_count} messages")
            cleanup_all_topics(redis)
            message_count = 0  # Reset counter
            continue
        
        data = json.loads(msg.value().decode("utf-8"))

        article_id = data.get("id")
        topic_ids = data.get("topic_ids", [])
        published_at = data.get("published_at")

        if not article_id or not topic_ids or not published_at:
            continue

        pub_epoch = to_epoch(published_at)
        now_epoch = int(time.time())

        # =========================
        # CHECK WINDOW 24H (THEO PH√öT)
        # =========================
        minutes_from_now = (now_epoch - pub_epoch) / 60
        if minutes_from_now > WINDOW_MINUTES:
            logger.info(f"[SKIP] Article {article_id} older than 24h")
            continue

        # =========================
        # PROCESS EACH TOPIC
        # =========================
        for topic_id in topic_ids:
            topic_key = f"topic:window:{topic_id}"

            # L∆∞u article v√†o window (score = epoch seconds)
            redis.zadd(topic_key, {str(article_id): pub_epoch})
            redis.expire(topic_key, REDIS_TTL_SECONDS)

            # X√ìA C√ÅC B√ÄI C≈® TR∆Ø·ªöC KHI T√çNH ƒêI·ªÇM
            cleanup_old_articles(redis, str(topic_id), now_epoch)

            # L·∫•y to√†n b·ªô b√†i c·ªßa topic (sau khi ƒë√£ cleanup)
            items = redis.zrange(topic_key, 0, -1, withscores=True)

            # N·∫øu topic ƒë√£ ch·∫øt ho√†n to√†n
            if not items:
                redis.delete(topic_key)
                redis.zrem("topic:score", str(topic_id))
                continue

            # =========================
            # T√çNH ƒêI·ªÇM DECAY
            # =========================
            total_score = 0.0
            for _, ts in items:
                minutes_diff = (now_epoch - ts) / 60
                if minutes_diff <= WINDOW_MINUTES:
                    total_score += decay_score(minutes_diff)

            total_score = round(total_score, 6)

            # L∆∞u score topic
            redis.zadd(
                "topic:score",
                {str(topic_id): total_score}
            )

            logger.info(
                f"[Topic {topic_id}] "
                f"articles={len(items)} "
                f"score={total_score}"
            )
        push_topic_scores(redis)
    except Exception as e:
        logger.exception(e)
