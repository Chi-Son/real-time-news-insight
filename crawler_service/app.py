import json ,feedparser,redis,hashlib, time, os,re
from shared.utils import url_to_hash
from shared.kafka_config import get_kafka_producer
# Káº¿t ná»‘i Ä‘áº¿n Redis
redis_host = os.getenv("REDIS_HOST", "localhost")  
r = redis.Redis(host=redis_host, port=6379, db=0, decode_responses=True)
#key cá»§a crawl trÃªn redis
REDIS_KEY = "crawled_urls"

# Ä‘áº·t topic Ä‘á»ƒ gá»­i vÃ o
producer=get_kafka_producer()
KAFKA_TOPIC="news.raw_links"

# Load json
def load_sources(path="rss_sources.json"):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

# Crawl vÃ  lá»c trÃ¹ng
def crawl_rss():
    sources = load_sources()
    for category, urls in sources.items():
        print(f"\nğŸ“° Category: {category}", flush=True)
        for url in urls:
            print(f"ğŸ”— Crawling: {url}", flush=True)
            feed = feedparser.parse(url)
            for entry in feed.entries[:3]:  
                link_hash = url_to_hash(entry.link)
                
                if r.sismember(REDIS_KEY, link_hash):
                    print(f"â›”ï¸ TrÃ¹ng: {entry.title}", flush=True)
                    continue
                #Ä‘oáº¡n nÃ y xá»­ lÃ½ trÃ¹ng gá»­i vÃ o kafka
                print(f"âœ… Má»›i: {entry.title}", flush=True)
                print(f"   {entry.link}", flush=True)
                r.sadd(REDIS_KEY, link_hash)
                r.expire(REDIS_KEY, 43200)
                data = {
                    "url": entry.link,
                    "title": entry.title,
                    "category": category,
                    "published": entry.published if "published" in entry else None
                }  
                producer.send(KAFKA_TOPIC,value=data)
                print("ÄÃ£ gá»­i")

if __name__ == "__main__":
    while True:
        print("â° Báº¯t Ä‘áº§u crawl RSS", flush=True)
        crawl_rss()
        print("ğŸ›Œ Ngá»§ 2 phÃºt 30...\n", flush=True)
        time.sleep(300)  
